{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc20374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.75\n",
      "Test Accuracy: 0.7647058823529411\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22812\\4175257474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m# Test the classify_text function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0minput_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''' anudheer lanjodku'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0mpredicted_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Predicted Label:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22812\\4175257474.py\u001b[0m in \u001b[0;36mclassify_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclassify_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# Preprocess the input text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mtext_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mtext_pad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\chatu\\Downloads\\vedscraaaa.csv\")\n",
    "\n",
    "\n",
    "X = data['content']\n",
    "y = data['value']\n",
    "\n",
    "data['content'].fillna('', inplace=True)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "model1 = MultinomialNB()\n",
    "model1.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_val_pred = model1.predict(X_val_tfidf)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "\n",
    "y_test_pred = model1.predict(X_test_tfidf)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "\n",
    "def classify_text(text):\n",
    "    # Preprocess the input text\n",
    "    text_seq = tokenizer.texts_to_sequences([text])\n",
    "    text_pad = pad_sequences(text_seq, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Make predictions\n",
    "    prediction = model1.predict(text_pad)\n",
    "    \n",
    "    # Return the class label (0 or 1)\n",
    "    return (np.rint(prediction[0][0]))\n",
    "\n",
    "# Test the classify_text function\n",
    "input_text = ''' anudheer lanjodku'''\n",
    "predicted_label = classify_text(input_text)\n",
    "print(\"Predicted Label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd83410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - 11s 2s/step - loss: 0.6917 - accuracy: 0.5676 - val_loss: 0.6797 - val_accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 1s 254ms/step - loss: 0.6766 - accuracy: 0.7432 - val_loss: 0.6652 - val_accuracy: 0.7500\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 1s 284ms/step - loss: 0.6618 - accuracy: 0.7432 - val_loss: 0.6488 - val_accuracy: 0.7500\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 1s 276ms/step - loss: 0.6383 - accuracy: 0.7297 - val_loss: 0.6283 - val_accuracy: 0.7500\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 1s 280ms/step - loss: 0.6118 - accuracy: 0.7432 - val_loss: 0.5991 - val_accuracy: 0.8750\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "data = pd.read_csv(r\"C:\\Users\\chatu\\Downloads\\vedscraaaa.csv\")\n",
    "\n",
    "# Handle missing values in the 'text' column (replace NaN with an empty string)\n",
    "data['content'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X = data['content']\n",
    "y = data['value']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 5000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to have consistent length\n",
    "max_sequence_length = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_sequence_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "# Build a Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=5, batch_size=64)\n",
    "\n",
    "# Function to classify text\n",
    "def classify_text(text, tokenizer):\n",
    "    # Preprocess the input text\n",
    "    text_seq = tokenizer.texts_to_sequences([text])\n",
    "    text_pad = pad_sequences(text_seq, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Make predictions\n",
    "    prediction = model.predict(text_pad)\n",
    "    \n",
    "    # Return the class label (0 or 1)\n",
    "    return int(round(prediction[0][0]))\n",
    "\n",
    "# Test the classify_text function\n",
    "input_text = \"This is a news article.\"\n",
    "predicted_label = classify_text(input_text, tokenizer)\n",
    "print(\"Predicted Label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910f949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.0\n",
      "Test Accuracy: 0.9411764705882353\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.75      0.86         4\n",
      "         1.0       0.93      1.00      0.96        13\n",
      "\n",
      "    accuracy                           0.94        17\n",
      "   macro avg       0.96      0.88      0.91        17\n",
      "weighted avg       0.95      0.94      0.94        17\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 3  1]\n",
      " [ 0 13]]\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Predicted Label: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load your dataset (assuming it's in a CSV file)\n",
    "data = pd.read_csv(r\"C:\\Users\\chatu\\Downloads\\vedscraaaa.csv\")\n",
    "\n",
    "# Handle missing values in the 'text' column (replace NaN with an empty string)\n",
    "data['content'].fillna('', inplace=True)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X = data['content']\n",
    "y = data['value']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Feature Extraction (TF-IDF Vectorization)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Create and train Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred = gnb.predict(X_val_tfidf.toarray())\n",
    "\n",
    "# Model Evaluation on Validation Set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation Accuracy:\", val_accuracy)\n",
    "\n",
    "# Predictions on test set\n",
    "y_test_pred = gnb.predict(X_test_tfidf.toarray())\n",
    "\n",
    "# Model Evaluation on Test Set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Print classification report and confusion matrix for more detailed evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "\n",
    "def classify_text(text, tokenizer):\n",
    "    # Preprocess the input text\n",
    "    text_seq = tokenizer.texts_to_sequences([text])\n",
    "    text_pad = pad_sequences(text_seq, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Make predictions\n",
    "    prediction = model.predict(text_pad)\n",
    "    \n",
    "    # Return the class label (0 or 1)\n",
    "    return int(round(prediction[0][0]))\n",
    "\n",
    "# Test the classify_text function\n",
    "input_text = '''From the Rough Riders of the Old West to the wandering yodel of Hank Williams, being a true outlaw is planted deep within the DNA of America. So when I was getting my ass tossed from Texas Roadhouse a week ago I knew that Outlaw Country was coming back and somewhere up in heaven Waylon and the boys were smiling down on me while lighting up a few cigs and blowing a few lines of the ole Devil’s Dandruff.\n",
    "\n",
    "The original outlaws of country music knew that their sound would never be accepted by the gatekeepers of Nashville. They were a little too rough, a little too ragged, a little too loud. That is exactly how I felt when I pulled into that Texas Roadhouse parking lot a week ago. The corporate big wigs had all their rules and regulations in place but big fucking deal. I knew they were never going to contain me. I was going to give the people what they wanted, whether they wanted it or not.\n",
    "\n",
    "Waylon, Willie, Johnny and the boys knew that before they stepped out on that stage they had to properly fuel up. I parked my ass at the bar and immediately ordered three shots of Everclear with three pickle backs.I knocked the shots back and settled in. I destroyed the complimentary bread and apple butter. I ordered another three shots and three picklebacks. I immediately went into the bathroom to vomit. I couldn’t be stopped, the country legends were speaking to me.\n",
    "\n",
    "I stumbled back to my barstool in true outlaw fashion, the vomit still hot on my breath. Nothing was going to stop me now. I lit up a cigarette and the barkeep told me “I’m sorry sir, there’s no smoking here.” Did he have any idea who he was talking to?\n",
    "\n",
    "I replied to him with a simple, “you think I give a fuck?”\n",
    "\n",
    "I then lit the napkin with my lighter. The embers from the cigarette and napkin were fueling my desire. There was no turning back now, I was a country legend. After I lit two more cigarettes much to the dismay of the bartender he apparently had enough and Security escorted me out.\n",
    "\n",
    "All in all it was a successful day for this country legend and I knew I had done my part to keep the tradition alive.'''\n",
    "predicted_label = classify_text(input_text, tokenizer)\n",
    "print(\"Predicted Label:\", predicted_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60672413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
